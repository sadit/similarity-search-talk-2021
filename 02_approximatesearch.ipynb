{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximate Similarity Search\n",
    "Eric S. Téllez <eric.tellez@infotec.mx>\n",
    "\n",
    "CONACYT-INFOTEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tackling with the curse of dimensionality\n",
    "\n",
    "It is impossible to obtain efficiently search algorithms under high dimensional data. However, to cope high dimensional datasets, we can relax the definitions of $knn$ as $knn^*$, just allowing to not retrieve the precise result set.\n",
    "\n",
    "Here we introduce the **recall** quality measure:\n",
    "$$\\frac{|knn(q, S) \\cap knn^*(q, S)|}{k} \\leq 1.0$$\n",
    "\n",
    "Hereafter, we will use $knn$ for both exact and approximate searches, and only distinguishing between them whenever is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic approaches\n",
    "- Aggressive prunning\n",
    "- Locality sensitive hashing (and related techniques)\n",
    "- Quantization and sketches based indexes\n",
    "- Permutation and neighborhood approximation indexes\n",
    "- Search graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aggressive prunning\n",
    "<img caption=\"Aggressive prunning\" heigth=\"90%\" src=\"aggressiveprunning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Locality Sensitive Hashing\n",
    "A family $H$ of functions $h : R^d \\rightarrow \\mathbb{N}$ is called $(P_1, P_2, r, cr)$-sensitive, if for any $p, q$:\n",
    "\n",
    "<div>if $D(p, q) < r$ then $Pr[h(p) = h(q)] > P_1$</div>\n",
    "<div>if $D(p, q) > cr$ then $Pr[h(p) = h(q)] < P_2$</div>\n",
    "\n",
    "#### Pros:\n",
    "- *Fast construction and fast searches*\n",
    "- *Low memory footprint*\n",
    "- There exists hashing functions for most popular distance functions\n",
    "\n",
    "#### Cons:\n",
    "- Slow for high quality setups\n",
    "- High memory for high quality setups\n",
    "- Hard to create specific hashing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantization and sketches based indexes\n",
    "\n",
    "<img width=\"50%\" src=\"voronoi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Permutation\n",
    "Let $P = p_1, p_2, \\cdots, p_m $ be a set of points called references.\n",
    "\n",
    "Use P to project the metric space into a *rank* space, that is, each object is represented by the order that *sees* each $p_i$, under the $d$ perspective.\n",
    "\n",
    "For example,\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td>\n",
    "$$u_1 = 3, 2, 1, 4, 5, 7, 6 $$\n",
    "$$u_2 = 1, 3, 2, 4, 5, 7, 6 $$\n",
    "$$u_3 = 4, 5, 3, 1, 2, 6, 7 $$</td>\n",
    "    <td><img src=\"refs.png\"></td>\n",
    " </tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neighborhood approximation indexes\n",
    "\n",
    "Considering only the closest references creates neighborhood approaches\n",
    "<table><tr>\n",
    "    <td><img src=\"knrvor.png\"></td>\n",
    "    <td><img src=\"knrballs.png\"></td>\n",
    " </tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Search graphs - Optimization heuristics\n",
    "\n",
    "<img src=\"delone.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Similarity search frameworks\n",
    "\n",
    "\n",
    "- <https://github.com/flann-lib/flann>\n",
    "- <https://github.com/nmslib/nmslib>\n",
    "- <https://github.com/FALCONN-LIB/FALCONN>\n",
    "- **<https://github.com/sadit/natix>**\n",
    "- **<https://github.com/sadit/SimilaritySearch.jl>**\n",
    "  - **<https://github.com/sadit/NeighborhoodApproximationIndex.jl>**\n",
    " \n",
    "NATIX and SimilaritySearch.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A brief comparison among indexes\n",
    "\n",
    "- Our experiments were performed on an Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz workstation with 16-core and 128 GiB of RAM, running Linux CentOS 7.\n",
    "- We do not use the multiprocessing capabilities in the search process, and both indexes and databases are maintained in the main memory.\n",
    "- We select synthetic and real benchmarks.\n",
    "- We report the performance of querying for 30 nearest neighbors for all datasets (recall and speed)\n",
    "\n",
    "_Tellez, E. S., Ruiz, G., Chavez, E., & Graff, M. (2021). A scalable solution to the nearest neighbor search problem through local-search methods on neighbor graphs. Pattern Analysis and Applications, 24(2), 763-777._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datasets:\n",
    "- **RAND**. Synthetic datasets in a unitary cube; these are standard benchmarks to study the algorithms’ performance for a fxed size and dimension. Four datasets of dimension 8, 16, 32, and 64; each dataset contain three million vectors. A query is solved by exhaustive search in 0.035, 0.042, 0.069, and 0.077 s, respectively. One thousand queries, not in datasets. Euclidean distance, i.e., L2.\n",
    "- **GIST-1M**. This database contains one million 960-dimensional feature vectors. We use the 1000 standard queries for this benchmark; the Euclidean distance, L2, is used for measuring the distance between point pairs. The average sequential search requires 0.385 s on our testing machine.\n",
    "- **SIFT-1M and SIFT-100M**. These datasets are subsets of the one billion dataset of scale-invariant feature transform (SIFT) descriptors. Each descriptor consists of a 128-dimensional vector. We use the 10,000 oficial queries, solved with the Euclidean distance, i.e., L2. Exhaustive search needs 2.540 and 0.024 s, in average, for SIFT100M and SIFT-1M, respectively.\n",
    "- **DEEPIMAGE-10M**. A subset of the one billion dataset from a deep learning-based image feature extraction. Each object is a 96-dimensional vector, and the distance notion is measured with the angle between vectors. The query collection has 10,000 vectors from the official query set; on average, each query is solved in 0.414 s using a bruteforce solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indexes to be compared\n",
    "#### We compare our indexes based on beam search (*BS*) and Iterated Hill Climbing (*IHC*) with *HNSW*, *PQ*, *KNR*, and *LSH*.\n",
    "\n",
    "\n",
    "_For *HNSW*, *PQ*, and *LSH*  we used the implementation of *FAISS*, written in C++; in particular, we use the cpu implementation. We also use *FALCONN-LSH* for benchmarking DEEPIMAGE._\n",
    "\n",
    "_We use our own implementation for BS, IHC, and KNR, written in the **Julia** language and it is also open-source.\n",
    "We also show the performance Seq, our brute-force search and Flat, the FAISS’s exhaustive search.\n",
    "implementation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img width=\"80%\" src=\"fig-vardim.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig-varn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig-sift-gist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig-largedatasets.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions and discussion (1/2)\n",
    "\n",
    "- About dimension and size\n",
    "- What about hardware?\n",
    " - multithreading computing\n",
    " - distributed computing\n",
    " - gpgpu\n",
    "- What is the best search index?\n",
    " - flexibility\n",
    " - memory resources\n",
    " - quality\n",
    "- Can be made faster?\n",
    "- Usage\n",
    " - Solving all knn (knn graph)\n",
    " - Clustering\n",
    " - Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions and discussion (2/2)\n",
    "\n",
    "- Directions\n",
    " - Computation of the neighborhood\n",
    " - Computation of an initial sample\n",
    " - Auto-tunning parameters dynamically\n",
    " - Sketches\n",
    " - Distributed computing\n",
    " - Demostrations and applications\n",
    " - Dynamic implementations\n",
    " - Secondary memory\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# My related work (1/2)\n",
    "- Tellez, E. S., Ruiz, G., Chavez, E., & Graff, M. (2021). A scalable solution to the nearest neighbor search problem through local-search methods on neighbor graphs. Pattern Analysis and Applications, 24(2), 763-777.\n",
    "- Hoyos, A., Ruiz, U., Chavez, E., & Tellez, E. S. (2020). Self-indexed motion planning. Information Systems, 87, 101399.\n",
    "- Ruiz, G., Chávez, E., Graff, M., & Téllez, E. S. (2015, October). Finding near neighbors through local search. In International Conference on Similarity Search and Applications (pp. 103-109). Springer, Cham.\n",
    "- Chávez, E., Graff, M., Navarro, G., & Téllez, E. S. (2015). Near neighbor searching with K nearest references. Information Systems, 51, 43-61.\n",
    "- Santoyo, F., Chavez, E., & Tellez, E. S. (2013, October). Compressing Locality Sensitive Hashing Tables. In 2013 Mexican International Conference on Computer Science (pp. 41-46). IEEE.\n",
    "- Tellez, E. S. (2012). Practical proximity searching in large metric databases (Doctoral dissertation, Ph. D. thesis, Universidad Michoacana de San Nicolás de Hidalgo, Morelia, Michoacán, Mexico)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# My related work (2/2)\n",
    "- Tellez, E. S., Chávez, E., & Navarro, G. (2011, June). Succinct nearest neighbor search. In Proceedings of the Fourth International Conference on SImilarity Search and APplications (pp. 33-40).\n",
    "- Chávez, E., & Tellez, E. S. (2010, September). Navigating k-nearest neighbor graphs to solve nearest neighbor searches. In Mexican Conference on Pattern Recognition (pp. 270-280). Springer, Berlin, Heidelberg.\n",
    "- Tellez, E. S., & Chavez, E. (2010, September). On locality sensitive hashing in metric spaces. In Proceedings of the Third International Conference on SImilarity Search and APplications (pp. 67-74)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
